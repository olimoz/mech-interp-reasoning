{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ArcAGI Challenge\n",
    "\n",
    "This notebook establishes an agentic workflow for an LLM to interrogate an ARC challenge's data, so self reflect on responses and finally, propose a predicted output for the challenge. \n",
    "\n",
    "This is done by adapting Microsoft Research's agentic framework, Autogen, with access to a Jupyter Notebook. In this implementaiton the agent is permitted to use code to access and view data, but cannot execute functions on the data, i.e. it has no meaningful access to code a solution.\n",
    "\n",
    "The code allows for the agent to loop over a number of the challenges in a stateless fashion. This means the agent has no memory, so it cannot learn techniques as it progresses over the challenges.\n",
    "\n",
    "Visit the ARC challenge at:\n",
    "\n",
    "    https://arcprize.org/guide\n",
    "\n",
    "Clone the data to local pc with git:\n",
    "\n",
    "    ```bash\n",
    "    cd myfolder/subfolder\n",
    "    git clone https://github.com/fchollet/ARC-AGI.git\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt Autogen to Execute Code in a Jupyter Notebook\n",
    "\n",
    "The following cells simply adapt Autogen's framework so agents can use a jupyter notebook which EXECUTES ON THE LOCAL PC and which can be saved for future inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from autogen import Agent, code_utils\n",
    "from pydantic import BaseModel\n",
    "from typing import Any, Dict, List, Literal, Optional\n",
    "\n",
    "# Build compatibility with future versions of AutoGen\n",
    "# See https://github.com/microsoft/autogen/pull/1405/files\n",
    "\n",
    "class CodeBlock(BaseModel):\n",
    "    \"\"\"A class that represents a code block.\"\"\"\n",
    "\n",
    "    \"\"\"The code to execute.\"\"\"\n",
    "    code: str\n",
    "\n",
    "    \"\"\"The language of the code.\"\"\"\n",
    "    language: str\n",
    "\n",
    "\n",
    "class CodeResult(BaseModel):\n",
    "    \"\"\"A class that represents the result of a code execution.\"\"\"\n",
    "\n",
    "    \"\"\"The exit code of the code execution.\"\"\"\n",
    "    exit_code: int\n",
    "\n",
    "    \"\"\"The output of the code execution.\"\"\"\n",
    "    output: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from re import escape, search, sub, DOTALL\n",
    "from queue import Empty\n",
    "from typing import List, Union\n",
    "from pydantic import BaseModel, Field\n",
    "from autogen.code_utils import extract_code\n",
    "from nbformat import write\n",
    "from nbformat.v4 import new_notebook, new_code_cell, new_output, new_markdown_cell\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "from jupyter_client import KernelManager\n",
    "from jupyter_client.kernelspec import NoSuchKernel, KernelSpecManager\n",
    "from autogen.code_utils import DEFAULT_TIMEOUT\n",
    "\n",
    "CODE_BLOCK_IDENTIFIER= \"```\"\n",
    "\n",
    "# Override the default timeout for code execution\n",
    "DEFAULT_TIMEOUT = 600\n",
    "\n",
    "# Note, this class should inherit from Pydantic's BaseModel, but making life easy for now...\n",
    "class NotebookCodeExecutor(object):\n",
    "    \"\"\"A code executor class that executes code statefully using a IPython kernel \n",
    "    operating with a Jupyter Notebook\n",
    "    Each execution is stateful and can access variables created from previous\n",
    "    executions in the same session.\n",
    "    \"\"\"\n",
    "\n",
    "    class UserCapability:\n",
    "        \"\"\"An AgentCapability class that gives agent ability use a Jupyter Notebook\n",
    "        code executor.\"\"\"\n",
    "\n",
    "        DEFAULT_SYSTEM_MESSAGE_UPDATE = \"\"\"You have been given coding capability\n",
    "to solve tasks using Python code in a stateful Jupyter Notebook\n",
    "When you write Python code, put the code in a block with the language set to Python.\n",
    "For example:\n",
    "\"\"\"+CODE_BLOCK_IDENTIFIER+\"\"\"python\n",
    "x = 3\n",
    "print(x)\n",
    "\"\"\"+CODE_BLOCK_IDENTIFIER+\"\"\"\n",
    "\n",
    "## Working with Jupyter Notebooks\n",
    "\n",
    "The code will be executed in a Jupyter Notebook, and the output will be returned to you.\n",
    "You can use variables created earlier in the subsequent code blocks.\n",
    "NEVER present your code in json format.\n",
    "If an error cannot be fixed or if the task is not solved even after the code is executed \n",
    "successfully, then analyze the problem, revisit your assumption, \n",
    "then pause to think of a different approach for solving the task.\n",
    "\n",
    "## Handling Charts\n",
    "\n",
    "When your code plots a chart then your chart will be presented in the notebook.\n",
    "BUT, charts presented in the notebook are inaccessible to you, you cannot view them.\n",
    "Therefore, prefer numerical methods over visuals for algorithm evaluation and optimisation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        def add_to_agent(self, agent):\n",
    "            \"\"\"Add this capability to an agent.\"\"\"\n",
    "            agent.update_system_message(agent.system_message + self.DEFAULT_SYSTEM_MESSAGE_UPDATE)\n",
    "\n",
    "    # default class variables\n",
    "    timeout = DEFAULT_TIMEOUT\n",
    "    kernel = \"python3\"\n",
    "    output_dir= \"notebooks\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # establish kernel\n",
    "        #self._shell = InteractiveShell.instance()\n",
    "        self._kernel_manager = KernelManager(kernel_name=self.kernel)\n",
    "        self._kernel_manager.start_kernel()\n",
    "        self._kernel_client = self._kernel_manager.client()\n",
    "        self._kernel_client.start_channels()\n",
    "        self._timeout = self.timeout\n",
    "\n",
    "        # establish notebook\n",
    "        self._nb = new_notebook()\n",
    "\n",
    "    # The notebook is useful as a public property, \n",
    "    # we can then inspect and modify the notebook as we wish\n",
    "    # in fact, we do so in the following example where we ...\n",
    "    # a) prefix the notebook with the team's task description in a markdown cell.\n",
    "    # b) we also want to disable warnings, so manually append a code cell which executes automatically\n",
    "    @property\n",
    "    def nb(self):\n",
    "        \"\"\"Returns the notebook for inspection\"\"\"\n",
    "        return self._nb\n",
    "\n",
    "    def nb_append_markdown(self, text: str) -> str:\n",
    "        \"\"\"Users may choose to append comments in a markdown cell\n",
    "        For example, the notebook makes more sense when prefixed with the task description\n",
    "        Args:\n",
    "            text (str): The text to append to the notebook as a markdown cell\n",
    "        \"\"\"\n",
    "        self._nb.cells.append(new_markdown_cell(text))\n",
    "        return 'markdown cell appended'\n",
    "\n",
    "    def nb_append_code(self, code: str) -> str:\n",
    "        \"\"\"Users may choose to append executable code in a code cell\n",
    "        For example, to disable warnings before the project starts, as warnings consume tokens\n",
    "        Args:\n",
    "            code (str): The code to execute in a code cell\n",
    "        \"\"\"\n",
    "        # Append the code block to the notebook     \n",
    "        code_block = CodeBlock(code=code, language=\"python\")\n",
    "\n",
    "        # execute the cell\n",
    "        result = self.execute_code_blocks([code_block])\n",
    "\n",
    "        # print result to user\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def user_capability(self) -> NotebookCodeExecutor.UserCapability:\n",
    "        \"\"\"Export a user capability that can be added to an agent.\"\"\"\n",
    "        return NotebookCodeExecutor.UserCapability()\n",
    "\n",
    "    def extract_code_blocks(self, message: str) -> List[CodeBlock]:\n",
    "        \"\"\"Extract code blocks from a message.\n",
    "        Args:\n",
    "            message (str): The message to extract code blocks from.\n",
    "        Returns:\n",
    "            List[CodeBlock]: The extracted code blocks.\n",
    "        \"\"\"\n",
    "        code_blocks = []\n",
    "        for lang, code in extract_code(message):\n",
    "            code_blocks.append(CodeBlock(code=code, language=lang))\n",
    "        return code_blocks\n",
    "\n",
    "    def execute_code_blocks(self, code_blocks: List[CodeBlock]) -> CodeResult:\n",
    "        \"\"\"For each code block, we will append it to the notebook as a cell\n",
    "            execute it then return the result.\n",
    "        Args:\n",
    "            code_blocks (List[CodeBlock]): The code blocks to execute.\n",
    "        Returns:\n",
    "            CodeResult: The result of the code execution.\n",
    "        \"\"\"\n",
    "        self._kernel_client.wait_for_ready(timeout=self._timeout)\n",
    "        outputs = []\n",
    "        for code_block in code_blocks:\n",
    "\n",
    "            # Ensure any mention of \"!pip install\" has the \"-qqq\" flag added\n",
    "            # this makes the pip install silent, which is important for the LLM\n",
    "            code = self._process_code(code_block.code)\n",
    "\n",
    "            # Append the code block to the notebook     \n",
    "            code_cell = new_code_cell(code)\n",
    "            self._nb.cells.append(code_cell)\n",
    "\n",
    "            # the cell we want to execute is now the final cell in the notebook\n",
    "            cell = self._nb.cells[-1]\n",
    "\n",
    "            # execute the cell\n",
    "            if cell.cell_type == 'code':\n",
    "                self._kernel_client.execute(cell.source, allow_stdin=False)\n",
    "                cell.outputs = []\n",
    "\n",
    "                # capture the result in the notebook\n",
    "                while True:\n",
    "                    try:\n",
    "                        msg = self._kernel_client.get_iopub_msg(timeout=self.timeout)\n",
    "                        msg_type = msg['msg_type']\n",
    "                        content = msg['content']\n",
    "\n",
    "                        if msg_type in ['execute_result', 'display_data']:\n",
    "                            cell.outputs.append(new_output(msg_type, data=content['data']))\n",
    "                        elif msg_type == 'stream':\n",
    "                            cell.outputs.append(new_output(msg_type, name=content['name'], text=content['text']))\n",
    "                        elif msg_type == 'error':\n",
    "                            cell.outputs.append(new_output(msg_type, ename=content['ename'], evalue=content['evalue'], traceback=content['traceback']))\n",
    "\n",
    "                        if msg_type == 'status' and content['execution_state'] == 'idle':\n",
    "                            break\n",
    "                    # handle time outs.\n",
    "                    except Empty:\n",
    "                        return CodeResult(\n",
    "                            exit_code=1,\n",
    "                            output=f\"ERROR: Timeout waiting for output from code block: {cell.source}\",\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        return CodeResult(exit_code=1, output=f\"ERROR: {e}\")\n",
    "\n",
    "                # we return images for display in the groupchat as a note, not the full image. The full image is kept in the Notebook only (see above)\n",
    "                # This is because the image is a lot of tokens, wastes money sending it to the LLM team.\n",
    "                modified_outputs = []\n",
    "                for output in cell.outputs:\n",
    "\n",
    "                    # determine whether output contains an image\n",
    "                    if output['output_type'] in ['execute_result', 'display_data']:\n",
    "                        output_is_image = any(key.startswith('image/') for key in output['data'])\n",
    "                    else:\n",
    "                        output_is_image = False\n",
    "\n",
    "                    # if it does contain an image, replace the image with a note for the returned value\n",
    "                    if output_is_image:\n",
    "                        modified_outputs.append(\"Charts are good practice but not visible. If using a chart to decide upon your next step, use a numerical method instead \")\n",
    "                    else:\n",
    "                        modified_outputs.append(output)  # Keep other outputs unchanged\n",
    "            else:\n",
    "                return CodeResult(\n",
    "                        exit_code=1,\n",
    "                        output=f\"ERROR: Attempted to execute a non-code cell: {cell.source}\"\n",
    "                        )\n",
    "\n",
    "            modified_outputs_joined = \"\\n\".join([str(modified_output) for modified_output in modified_outputs])\n",
    "            outputs.append(modified_outputs_joined)\n",
    "\n",
    "        return CodeResult(exit_code=0, output=\"\\n\".join([str(output) for output in outputs]))\n",
    "\n",
    "    def save_notebook(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Saves the current notebook to the specified folder and filename.\n",
    "        Intended to be used when groupchat has completed, user is expected to save the notebook to their local machine.\n",
    "\n",
    "        Args:\n",
    "            file_path: The file path (inc file name) where the notebook is located.\n",
    "        Returns:\n",
    "            A status message indicating success or failure.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                write(self._nb, f)\n",
    "            return \"Notebook saved successfully.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error saving notebook: {str(e)}\"\n",
    "\n",
    "    def restart(self) -> None:\n",
    "        \"\"\"Restart a new session.\"\"\"\n",
    "        self._kernel_client.stop_channels()\n",
    "        self._kernel_manager.shutdown_kernel()\n",
    "        self._kernel_manager = KernelManager(kernel_name=self.kernel)\n",
    "        self._kernel_manager.start_kernel()\n",
    "        self._kernel_client = self._kernel_manager.client()\n",
    "        self._kernel_client.start_channels()\n",
    "        # print result to user\n",
    "        print(f\"Notebook kernel has been restarted, kernel name={self.kernel}\")\n",
    "\n",
    "    def shutdown(self) -> None:\n",
    "        \"\"\"Shutdown the notebook\"\"\"\n",
    "        self._kernel_client.stop_channels()\n",
    "        self._kernel_manager.shutdown_kernel()\n",
    "        # print result to user\n",
    "        print(\"Notebook kernel has been shutdown\")\n",
    "\n",
    "    def _process_code(self, code: str) -> str:\n",
    "        \"\"\"Process code before execution.\"\"\"\n",
    "        # Find lines that start with `! pip install` and make sure \"-qqq\" flag is added.\n",
    "        lines = code.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            # use regex to find lines that start with `! pip install` or `!pip install`.\n",
    "            match = search(r\"^! ?pip install\", line)\n",
    "            if match is not None:\n",
    "                if \"-qqq\" not in line:\n",
    "                    lines[i] = line.replace(match.group(0), match.group(0) + \" -qqq\")\n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeExecutorFactory:\n",
    "    \"\"\"A factory class for creating code executors.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create(code_execution_config: Dict) -> NotebookCodeExecutor:\n",
    "        \"\"\"Get a code executor based on the code execution config.\"\"\"\n",
    "        executor_name = code_execution_config.get(\"executor\")\n",
    "        if executor_name == \"notebook\":\n",
    "            return NotebookCodeExecutor(**code_execution_config.get(\"notebook\", {}))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown code executor {executor_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbformat.v4 import new_markdown_cell\n",
    "from autogen.code_utils import UNKNOWN\n",
    "\n",
    "def _generate_code_execution_reply_using_executor(\n",
    "    self,\n",
    "    messages: Optional[List[Dict]] = None,\n",
    "    sender: Optional[Agent] = None,\n",
    "    config: Optional[Union[Dict, Literal[False]]] = None,\n",
    "    ):\n",
    "\n",
    "    \"\"\"Generate a reply using code executor.\n",
    "\n",
    "    Processes messages, performs notebook operations and execute code therein, based on the extracted intent.\n",
    "\n",
    "    This function iterates through a specified number of recent messages, extracts any code blocks and text content, \n",
    "    and appends bith comments and code to a Jupyter notebook. \n",
    "\n",
    "    The method first checks the configuration for code execution. If disabled, it immediately returns without processing. \n",
    "    For each groupchat message it extracts any embedded code blocks and text content. \n",
    "    Note, a single groupchat message may contain multiple code blocks and text content.\n",
    "    The text is grouped together and appended to the notebook. The code is appended and executed in as many chunks as it is provided. \n",
    "\n",
    "    The function returns a boolean indicating whether any notebook operation was performed and a message detailing \n",
    "    the outcome of the operation, such as successful execution of cells or error.\n",
    "\n",
    "    Args:\n",
    "        messages: list of message dicts (aka a groupchat), where each message contains content that may be code and/or text.\n",
    "        sender  : The sender of the messages.\n",
    "        config  : Configuration options for code execution\n",
    "    Returns\n",
    "        (bool), (bool) : is final message?, reply content\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    code_execution_config = config if config is not None else self._code_execution_config\n",
    "    if code_execution_config is False:\n",
    "        return False, None\n",
    "    if messages is None:\n",
    "        messages = self._oai_messages[sender]\n",
    "    last_n_messages = code_execution_config.get(\"last_n_messages\", \"auto\")\n",
    "\n",
    "    if not (isinstance(last_n_messages, (int, float)) and last_n_messages >= 0) and last_n_messages != \"auto\":\n",
    "        raise ValueError(\"last_n_messages must be either a non-negative integer, or the string 'auto'.\")\n",
    "\n",
    "    messages_to_scan = last_n_messages\n",
    "    if last_n_messages == \"auto\":\n",
    "        # Find when the agent last spoke\n",
    "        messages_to_scan = 0\n",
    "        for i in range(len(messages)):\n",
    "            message = messages[-(i + 1)]\n",
    "            if \"role\" not in message:\n",
    "                break\n",
    "            elif message[\"role\"] != \"user\":\n",
    "                break\n",
    "            else:\n",
    "                messages_to_scan += 1\n",
    "\n",
    "    # iterate through the last n messages in reverse\n",
    "    # if code blocks are found, execute the code blocks and return the output\n",
    "    # if no code blocks are found, continue\n",
    "    for i in range(min(len(messages), messages_to_scan)):\n",
    "        message = messages[-(i + 1)]\n",
    "        if not message[\"content\"]:\n",
    "            continue\n",
    "        \n",
    "        # identify code blocks in the message\n",
    "        code_blocks = self._code_executor.extract_code_blocks(message[\"content\"])\n",
    "        if len(code_blocks) == 1 and code_blocks[0].language == UNKNOWN:\n",
    "            continue\n",
    "\n",
    "        # Retain agent comments as markdown cells\n",
    "        # This helps the notebook to explain the agent's reasoning behind the code\n",
    "        # The text content for a markdown cell is the message content with the code blocks removed  \n",
    "        pattern = rf\"(?<!\\\\){CODE_BLOCK_IDENTIFIER}.*?(?<!\\\\){CODE_BLOCK_IDENTIFIER}\"\n",
    "\n",
    "        # Replace code blocks with a break\n",
    "        text_content = sub(pattern, \"<br>\", message[\"content\"], flags=DOTALL).strip()\n",
    "\n",
    "        # if text content is provided then append it to notebook as a markdown cell\n",
    "        # Note, we do this before appending or executing any code cells\n",
    "        if len(text_content)>0:\n",
    "            text_cell = new_markdown_cell(text_content)\n",
    "            self._code_executor._nb.cells.append(text_cell)\n",
    "        \n",
    "        # we don't need to execute the markdown cell\n",
    "        # however, we do need to append the code blocks as code cells, execute them and gather outputs\n",
    "        code_result = self._code_executor.execute_code_blocks(code_blocks)\n",
    "        exitcode2str = \"execution succeeded\" if code_result.exit_code == 0 else \"execution failed\"\n",
    "        return True, f\"exitcode: {code_result.exit_code} ({exitcode2str})\\nCode output: {code_result.output}\"\n",
    "\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the current version of autogen's ConversableAgent class\n",
    "# but wish to replicate a future version which has the _generate_code_execution_reply_using_executor method\n",
    "from autogen import ConversableAgent\n",
    "ConversableAgent._generate_code_execution_reply_using_executor = _generate_code_execution_reply_using_executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the NotebookAgent class which will execute code in a notebook on our behalf, within the AutoGen framework\n",
    "from autogen import UserProxyAgent\n",
    "\n",
    "class NotebookAgent(UserProxyAgent):\n",
    "\n",
    "    def __init__(self, \n",
    "                 name,\n",
    "                 system_message,\n",
    "                 code_execution_config, \n",
    "                 function_map = None):\n",
    "\n",
    "        super().__init__(\n",
    "            name            = name,\n",
    "            system_message  = system_message,\n",
    "            llm_config      = False,\n",
    "            human_input_mode= \"NEVER\",\n",
    "            is_termination_msg=lambda msg: \"TERMINATE\" in msg.get(\"content\"),\n",
    "            function_map    = function_map,\n",
    "            max_consecutive_auto_reply=10,\n",
    "            )\n",
    "\n",
    "        self._code_execution_config = code_execution_config\n",
    "\n",
    "        # Create a code executor based on the code execution config\n",
    "        self._code_executor = CodeExecutorFactory.create(self._code_execution_config)\n",
    "\n",
    "        # Append the code executor capability to this agent's system prompt\n",
    "        self._code_executor.user_capability.add_to_agent(self)\n",
    "\n",
    "        # Ensure code executor responses (i.e. output from code execution) reach the conversation\n",
    "        self.register_reply(ConversableAgent, \n",
    "                            ConversableAgent._generate_code_execution_reply_using_executor)\n",
    "\n",
    "    @property\n",
    "    def code_executor(self) -> NotebookCodeExecutor:\n",
    "        \"\"\"The code executor used by this agent. Raise if code execution is disabled.\"\"\"\n",
    "        if not hasattr(self, \"_code_executor\"):\n",
    "            raise ValueError(\n",
    "                \"No code executor as code execution is disabled. \"\n",
    "                \"To enable code execution, set code_execution_config.\"\n",
    "            )\n",
    "        return self._code_executor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Data\n",
    "\n",
    "Having adapted Autogen, we need functions to access the data from file.\n",
    "First step is to download the ARC challenge files\n",
    "Source is at: https://github.com/fchollet/ARC-AGI/tree/master/data/training\n",
    "\n",
    "Examples of these JSON files have been downloaded and can be found in the folder '/data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import pprint \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from autogen import config_list_from_json, AssistantAgent\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import shapely\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data access functions\n",
    "\n",
    "def load_data(file_path: str) -> Dict[str, Any]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        return \n",
    "\n",
    "def get_train(file_path: str, pair=0, print_to_screen=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = json.load(file)['train']\n",
    "\n",
    "    train_len = len(dataset)\n",
    "\n",
    "    if pair < 0 :\n",
    "        pair = 0\n",
    "    else:\n",
    "        pair = min(train_len,pair)\n",
    "\n",
    "    array_input  = np.array(dataset[pair]['input'])\n",
    "    array_output = np.array(dataset[pair]['output'])\n",
    "\n",
    "    if print_to_screen:\n",
    "        print(f\"There are {train_len} pairs of input and output in the training dataset.\")\n",
    "        print(f\"Pair {pair}\")\n",
    "        print(f\"INPUT. Shape={array_input.shape}\")\n",
    "        pprint.pprint(array_input)\n",
    "        print(f\"OUTPUT. Shape={array_output.shape}\")\n",
    "        pprint.pprint(array_output)\n",
    "\n",
    "    return array_input, array_output\n",
    "\n",
    "def output_less_input(input, output, print_to_screen=True):\n",
    "    \"\"\"\n",
    "    Compare two np.arrays of integers, input and output, \n",
    "    confirm same shape, \n",
    "    then calculate integer difference (output-input).\n",
    "    \"\"\"\n",
    "    if input.shape != output.shape:\n",
    "        print(\"Grids have different shapes.\")\n",
    "        return None\n",
    "    else:\n",
    "        diff = output - input\n",
    "        if print_to_screen:\n",
    "            print(f\"Output - Input = Difference\")\n",
    "            pprint.pprint(diff)\n",
    "        return diff\n",
    "\n",
    "\n",
    "def get_test(file_path: str, print_to_screen=True, testtype='input'):\n",
    "    \"\"\"\n",
    "    datatype can be input or output \n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = json.load(file)\n",
    "\n",
    "    grid_test = np.array(dataset['test'][0][testtype])\n",
    "\n",
    "    if print_to_screen:\n",
    "        print(f\"TEST {testtype}. Shape={grid_test.shape}\")\n",
    "        pprint.pprint(grid_test)\n",
    "\n",
    "    return grid_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above functions will be made avaialable to the agents in the notebook. They will not see the code, the cells are executed before the notebook is provided to the agent. The resul is that the agents can easily open challenge files using a function name which is given to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notebook_setup(filename:str, train_or_eval:str):\n",
    "\n",
    "    # Disable version warnings in the notebook, we pay for the tokens and don't want to waste them on warnings\n",
    "    # exit code=0 means success, exit code=1 means failure\n",
    "    disable_warnings_code = \"\"\"\n",
    "import warnings\n",
    "\n",
    "# Filter out FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \"\"\"\n",
    "\n",
    "    # functions to load data\n",
    "    load_data_function = \"\"\"\n",
    "\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Ensure the large matrices print without carriage returns.\n",
    "np.set_printoptions(linewidth=150)\n",
    "\n",
    "def get_train(file_path: str, pair=0, print_to_screen=True):\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = json.load(file)['train']\n",
    "\n",
    "    train_len = len(dataset)\n",
    "\n",
    "    if pair < 0 :\n",
    "        pair = 0\n",
    "    else:\n",
    "        pair = min(train_len,pair)\n",
    "\n",
    "    array_input  = np.array(dataset[pair]['input'])\n",
    "    array_output = np.array(dataset[pair]['output'])\n",
    "\n",
    "    if print_to_screen:\n",
    "        print(f\"There are {train_len} pairs of input and output in the training dataset.\")\n",
    "        print(f\"Pair {pair}\")\n",
    "        print(f\"INPUT. Shape={array_input.shape}\")\n",
    "        pprint.pprint(array_input)\n",
    "        print(f\"OUTPUT. Shape={array_output.shape}\")\n",
    "        pprint.pprint(array_output)\n",
    "\n",
    "    return array_input, array_output\n",
    "\n",
    "def output_less_input(output, input, print_to_screen=True):\n",
    "    \n",
    "    # Compare two np.arrays of integers, input and output, \n",
    "    # confirm same shape, \n",
    "    # then calculate integer difference (output-input).\n",
    "\n",
    "    if input.shape != output.shape:\n",
    "        print(\"Grids have different shapes.\")\n",
    "        return None\n",
    "    else:\n",
    "        diff = output - input\n",
    "        if print_to_screen:\n",
    "            print(f\"Output - Input = Difference\")\n",
    "            pprint.pprint(diff)\n",
    "        return diff\n",
    "\n",
    "def get_test(file_path: str, print_to_screen=True):\n",
    "\n",
    "    # datatype can be input or output \n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = json.load(file)\n",
    "\n",
    "    grid_test = np.array(dataset['test'][0]['input'])\n",
    "\n",
    "    if print_to_screen:\n",
    "        print(f\"TEST Input. Shape={grid_test.shape}\")\n",
    "        pprint.pprint(grid_test)\n",
    "\n",
    "    return grid_test\n",
    "\n",
    "test_counter = 0\n",
    "\n",
    "def test_outcome(file_path: str, test_prediction):\n",
    "\n",
    "    global test_counter\n",
    "\n",
    "    # Load the dataset from the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = json.load(file)\n",
    "\n",
    "    # Extract the actual test output from the dataset\n",
    "    test_actual = np.array(dataset['test'][0]['output'])\n",
    "\n",
    "    # Compare test_prediction with test_actual, if we have used 3 or less tries\n",
    "    if test_counter >= 3:\n",
    "        print(f\"You have no more tries, save your prediction and state the termination word.\")\n",
    "        test_outcome = None\n",
    "    else:\n",
    "        print(f\"Prediction was {np.array_equal(test_prediction, test_actual)}\")\n",
    "        test_outcome = np.array_equal(test_prediction, test_actual)\n",
    "    \n",
    "    test_counter += 1\n",
    "\n",
    "    return test_outcome\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    load_data = f\"\"\"\n",
    "\n",
    "# load challenge data from the file\n",
    "import os\n",
    "\n",
    "# set working directory\n",
    "os.chdir(\"/home/oliver/Documents/LangChain/ProductDevelopment/AutoGen/ArcAGI\")\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "# get data file\n",
    "file_path = 'data/{train_or_eval}/{filename}'\n",
    "\n",
    "# load and print the first instance of the training data\n",
    "input0, output0 = get_train(file_path, pair=0, print_to_screen=False)\n",
    "\n",
    "# note, to load the second instance of the training data we would write...\n",
    "# we can optionally view the grid data by setting print_to_screen=True\n",
    "# input1, output1 = get_train(file_path, pair=1, print_to_screen=False)\n",
    "    \"\"\"\n",
    "\n",
    "    return disable_warnings_code, load_data_function, load_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the task for the agents to complete, this is effectively the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_task(filename, train_len, train_or_eval):\n",
    "\n",
    "    ## Task description\n",
    "    task_description = f\"\"\"\n",
    "# PROJECT INSTRUCTIONS\n",
    "\n",
    "The training data consists of pairs of input and output grids, presented as numpy arrays of varying shapes.\n",
    "Your task is to discover the single mapping which converts each input grid to its corresponding output grid and apply that to the test input, arriving at a test output.\n",
    "\n",
    "## 1. OBSERVE AND HYPOTHESISE THE MAPPING LOGIC FOR ALL TRAINING PAIRS\n",
    "\n",
    "When building your hypotheses on the mappings, be aware of the following common transformations:\n",
    "\n",
    "    Grid Expansion and Repetition (Tiling):\n",
    "    - Simply expand the grid and repeat (tile) the input grid into the output grid\n",
    "    Symmetry and Mirroring (flipping):\n",
    "    - Horizontally or vertically\n",
    "    Propagation of patterns:\n",
    "    - Identify non-zero clusters or shapes in the input grid and propagating them in the output. Proceeding horizontally, vertically or diagonally.\n",
    "    Mathematical Operations:\n",
    "    - Incrementing values, taking modulo, or performing addition.\n",
    "    Color/Value Substitution:\n",
    "    - Values in the input grid replaced with different values in the output grid, often changing all instances of one number to another\n",
    "    Shape Detection and Transformation:\n",
    "    - Identifying geometric shapes in the input grid and applying transformations such as rotation, scaling, flipping, translation and/or overlapping.\n",
    "    Grid Segmentation:\n",
    "    - Divide the input grid into sections and apply transformations to each section.\n",
    "    Boundary Detection and Fill:\n",
    "    - Identify the boundaries of shapes or patterns and fill them with specific values. This sometimes involved propagating values from the edges inward.\n",
    "    Connectivity-based Transformations:\n",
    "    - Using connected component analysis to identify and transform groups of connected cells.\n",
    "    Rule-based Transformations:\n",
    "    - Applying specific rules based on the arrangement of values in the input grid. These rules often considered the neighboring cells of each position.\n",
    "    Coordinate-based Transformations:\n",
    "    - Using the coordinates of cells to determine how they should be transformed or moved in the output grid.\n",
    "    When the pattern is more complex than originally assumed:\n",
    "    - Review all training pairs again and try to describe the transformation in plain language\n",
    "\n",
    "Please proceed with developing your own hypotheses on the training data.\n",
    "\n",
    "## 2. PREDICT THE OUTPUT GRID FOR THE TEST INPUT GRID\n",
    "\n",
    "AFTER having observed and hypothesised the mapping logic for ALL training pairs, \n",
    "then view the test input grid and predict the output grid.\n",
    "Access the test grid using Python code (see below), but DO NOT USE CODE TO PREDICT the output, SIMPLY ESTIMATE IT.\n",
    "\n",
    "IMPORTANT: Present your prediction in np.array format.\n",
    "\n",
    "    ### NOTE ON ACCESS TO TRAINING DATA\n",
    "\n",
    "    The coding environment has been established for you. \n",
    "    You can access and view the initial input and output training grids like this:\n",
    "\n",
    "        input_train0, output_train0 = get_train(file_path, pair=0, print_to_screen=True)\n",
    "\n",
    "    You therefore have access to the initial training pair, which has already been printed to screen:\n",
    "    - input_train0: the input array\n",
    "    - output_train0: the output array\n",
    "\n",
    "    Then access and view subsequent pairs of training grids by changing \n",
    "    the pair number in the get_train function and exeucting code like this:\n",
    "\n",
    "        input_train1, output_train1 = get_train(file_path, pair=1, print_to_screen=True)\n",
    "\n",
    "    There are {train_len} training tasks to complete, each with a different pair of input and output grids.\n",
    "    You can compare training grids and view the difference like this:\n",
    "\n",
    "        diff = output_less_input(output_grid, input_grid, print_to_screen=True)\n",
    "\n",
    "    DO NOT PROCEED to the test grid until you have successfully mapped ALL FOUR \n",
    "    training grids to their corresponding output grids.\n",
    "\n",
    "    ### NOTE ON ACCESS TO TEST DATA\n",
    "\n",
    "    You can access and view the final test grid as follows, assuming you wish to print_to_screen:\n",
    "        input_test = get_test('data/{train_or_eval}/{filename}', print_to_screen=True)\n",
    "\n",
    "    Execute your function on the test grid to predict the output grid.\n",
    "\n",
    "## 4. SAVE YOUR TEST PREDICTION THEN END THE CONVERSATION\n",
    "\n",
    "Save your final prediction in numpy array as text, like this:\n",
    "\n",
    "    np.savetxt('predictions/{train_or_eval}/{filename}_output_test.txt', output_test, fmt='%d', delimiter=',')\n",
    "\n",
    "## 5. END THE CONVERSATION\n",
    "\n",
    "When the project is complete OR you have submitted three false predictions, then you must end the conversation with the termination word.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return task_description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the autogen framework the notebook environment is also labelled as an 'agent', however it is not an LLM. The agent is just code, the codee which we created at the start of this notebook to enable autogen to access a Jupyter notebook and execute code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook\n",
    "# This is the executor who will be given ability to execute code in a Jupyter Notebook\n",
    "\n",
    "def notebook_create():\n",
    "\n",
    "    # Instantiate the Notebook\n",
    "    notebook = NotebookAgent(\n",
    "        name=\"Notebook\",\n",
    "        # We include a system message to explain the capabilities of the notebook to the GroupChatManager (if any) which is an LLM, whereas this agent is not an LLM\n",
    "        system_message=\"\"\"Notebook. You are a Jupyter Notebook\n",
    "        When presented with python code wrapped in this delimiter '```' then you execute that code in a Jupyter Notebook and report the results back to team.\n",
    "        \"\"\",\n",
    "        code_execution_config={ \"last_n_messages\": 3, \n",
    "                                \"work_dir\"       : \"coding\",        \n",
    "                                \"use_docker\"     : False,     # set to True or image name like \"python:3\" to use docker\n",
    "                                \"executor\"       : \"notebook\" # this is crucial, ensures execution happens in a notebook\n",
    "                                },\n",
    "        #function_map = {\"myfunction\": myfunction_json}\n",
    "    )\n",
    "\n",
    "    return notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM agent will be labelled a 'data scientist' and given a system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datascientist_create(model_name):\n",
    "\n",
    "    # filter LLM API keys for the desired model\n",
    "    datascientist_config_list= config_list_from_json(\n",
    "        \"MODEL_CONFIG_LIST\",\n",
    "        filter_dict={\n",
    "            \"model\": {model_name} \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # prep configuration for the coding agent to use the selected LLM and its API key\n",
    "    datascientist = AssistantAgent(\n",
    "        name=\"DataScientist\",\n",
    "        llm_config={\"seed\"            : 42,  # change the seed for different trials\n",
    "                    \"temperature\"     : 0,   # 0 uses most likely token every time, highly repeatable. 1 is more creative.\n",
    "                    \"config_list\"     : datascientist_config_list,\n",
    "                    #\"request_timeout\" : 5*60,\n",
    "                    },\n",
    "        human_input_mode=\"NEVER\",\n",
    "        is_termination_msg=lambda msg: \"TERMINATE\" in msg.get(\"content\"),#lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"), \n",
    "        system_message=\"\"\"# YOU ARE A PUZZLE SPECIALIST\n",
    "\n",
    "You have special observational and hypothecation abilities to propose rules which govern mapping patterns in data.\n",
    "You are intuitive, perceptive, sensing the bigger picture. You are excellent at using your observations to make predictions.\n",
    "\n",
    "## BEST PRACTICES FOR CODING\n",
    "\n",
    "You can write code but only to access or save files, you cannot write code for any other purpose.\n",
    "* Present only one block of code at a time. **Do not present multiple blocks of code in a single message.**\n",
    "Allow yourself to see output from executed before presenting the next block of code.\n",
    "Code should be written incrementally, and you should leverage the statefulness of the kernel to avoid repeating code.\n",
    "Code MUST be presented in a code block wrapped in three backticks, '```' with the language set to Python.\n",
    "For example:\n",
    "```python\n",
    "x = 3\n",
    "```\n",
    "\n",
    "## WHEN PROJECT IS COMPLETE\n",
    "\n",
    "When the project is complete, as specified in the project instructions, then you must say 'TERMINATE', which is the termination word.\n",
    "You partner may continue regardless of your termination, simply repeat 'TERMINATE'\n",
    "\n",
    "            \"\"\",\n",
    "    )\n",
    "\n",
    "    return datascientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to keep track of the challeng outcomes as we iterate over them. Here we save them to a spreadsheet which must be created as a blank file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the results in a table on file\n",
    "def outcome_spreadsheet(train_or_eval, filename, outcome, date_time_end_label, model_name, est_method, outcomes_path='arcagi_outcomes.xlsx'):\n",
    "    # Define the file path\n",
    "    outcomes_path = 'arcagi_outcomes.xlsx'\n",
    "\n",
    "    # New data to append\n",
    "    new_data = pd.DataFrame({\n",
    "        'dataset': [train_or_eval],\n",
    "        'file'   : [filename],\n",
    "        'outcome': [outcome],\n",
    "        'time'   : [date_time_end_label],\n",
    "        'model'  : [model_name],\n",
    "        'est_method': [est_method],\n",
    "    })\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(outcomes_path):\n",
    "\n",
    "        print('Arranging updates for spreadsheet')\n",
    "\n",
    "        # Load existing data\n",
    "        existing_data = pd.read_excel(outcomes_path)\n",
    "\n",
    "        # Append new data\n",
    "        combined_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        print('Arranging first data for spreadsheet')\n",
    "\n",
    "        # If file doesn't exist, use only the new data\n",
    "        combined_data = new_data\n",
    "\n",
    "    # Save the combined data to Excel\n",
    "    print('Updating spreadsheet:', outcomes_path)\n",
    "    combined_data.to_excel(outcomes_path, index=False)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def notebook_outcome_stats(notebook, filename, file_path, train_or_eval, date_time_start, cwd, model_name, est_method):\n",
    "    # Record the outcome\n",
    "\n",
    "    # Record time finished\n",
    "    date_time_end = datetime.datetime.now()\n",
    "    date_time_end_label = date_time_end.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    duration = date_time_end - date_time_start\n",
    "\n",
    "    # Extract hours, minutes, and seconds\n",
    "    hours, remainder = divmod(duration.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    duration_txt     = f\"Duration of notebook execution\\n {hours} hours, {minutes} minutes, {seconds} seconds\"\n",
    "\n",
    "    # Record duration in the notebook itself\n",
    "    notebook.code_executor.nb_append_markdown(f\"### HUMAN: {duration_txt}\")\n",
    "\n",
    "    # Check the results\n",
    "    actual_test = get_test(file_path, testtype='output', print_to_screen=False)\n",
    "    predicted_test_file = os.path.join(cwd, 'predictions', train_or_eval, f\"{filename}_output_test.txt\")\n",
    "    \n",
    "    if os.path.exists(predicted_test_file):\n",
    "        predicted_test = np.loadtxt(predicted_test_file, delimiter=',', dtype=int)\n",
    "\n",
    "        if actual_test.shape != predicted_test.shape:\n",
    "            print(f\"Warning: Shape mismatch. actual_test shape: {actual_test.shape}, predicted_test shape: {predicted_test.shape}\")\n",
    "            outcome = False\n",
    "        else:\n",
    "            outcome = (actual_test == predicted_test).all()\n",
    "    else:\n",
    "        print(f\"Warning: Predicted test file {predicted_test_file} not found.\")\n",
    "        outcome = \"Incomplete\"\n",
    "\n",
    "    # Save outcome to the notebook itself\n",
    "    notebook.code_executor.nb_append_markdown(\"### HUMAN: End of Cells by AI Agents\\n Let's test the results of their prediction.\")\n",
    "    notebook.code_executor.nb_append_code(f\"print('Prediction is accurate? ', {outcome})\")\n",
    "\n",
    "    # Save the notebook for later inspection\n",
    "    # Creates unique filename so this notebook can be repeatedly executed and team solutions compared\n",
    "    filename_nb = f\"{filename}_notebook_{date_time_end_label}.ipynb\"\n",
    "    notebook.code_executor.save_notebook(file_path=os.path.join(cwd, 'predictions', train_or_eval, filename_nb))\n",
    "\n",
    "    # Record stats to spreadsheet\n",
    "    outcome_spreadsheet(train_or_eval, filename, outcome, date_time_end_label, model_name, est_method)\n",
    "\n",
    "    # print outcome stats\n",
    "    print(f\"Duration : {duration_txt}\")\n",
    "    print(f\"Prediction is accurate? : {outcome}\\n\")\n",
    "\n",
    "    if outcome != 'Incomplete':\n",
    "        _ = output_less_input(actual_test, predicted_test, print_to_screen=True)\n",
    "\n",
    "    return str(outcome), date_time_end, duration_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_files(folder_path):\n",
    "    file_count = 0\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "    return file_count\n",
    "\n",
    "def get_files(folder_path, num_files, sort_method):\n",
    "    \"\"\"\n",
    "    Retrieve files from a folder in alphabetical or random order.\n",
    "    \n",
    "    Args:\n",
    "    folder_path (str): Path to the folder containing the files.\n",
    "    num_files (int, optional): Number of files to retrieve. If None, retrieves all files.\n",
    "    sort_method (str, optional): Method to sort the files. Can be 'alphabetical' or 'random'. \n",
    "                            Defaults to 'alphabetical'.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of file names sorted according to the specified method.\n",
    "    \"\"\"\n",
    "    # Get all files in the folder\n",
    "    all_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if sort_method == 'alphabetical':\n",
    "        # Sort the files alphabetically\n",
    "        all_files.sort()\n",
    "    elif sort_method == 'random':\n",
    "        # Shuffle the files randomly\n",
    "        random.shuffle(all_files)\n",
    "    #else do nothing, return the files as they are\n",
    "   \n",
    "    # Return the specified number of files or all if num_files is None\n",
    "    return all_files[:num_files] if num_files is not None else all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main loop. We iterate over a selected number of files, as set by num_files. This could be set to the entire  challenges, but rate limits may prevent this many being executed in a 24hr period. The below code sets challenges for the current batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## MAIN\n",
    "\n",
    "# Working directory and environment keys\n",
    "os.chdir(\"/home/oliver/Documents/LangChain/ProductDevelopment/AutoGen/ArcAGI\")\n",
    "cwd = os.getcwd()\n",
    "train_or_eval = 'training' # evaluation training\n",
    "\n",
    "# read local .env file for LLM API keys\n",
    "_ = load_dotenv(find_dotenv(usecwd=True))\n",
    "oai_config_value = os.environ.get('MODEL_CONFIG_LIST')\n",
    "\n",
    "# How many examples to work thru? \n",
    "num_files = 20\n",
    "\n",
    "# What LLM will we use?\n",
    "model_name = \"claude-3-5-sonnet-20240620\" # \"claude-3-5-sonnet-20240620\", \"gpt-4o\", \"gpt-4-1106-preview\"\n",
    "\n",
    "# get the examples to work on\n",
    "source_folder = os.path.join('data', train_or_eval)\n",
    "file_qty = count_files(source_folder)\n",
    "print(f\"Number of files in the folder: {file_qty}\")\n",
    "\n",
    "filenames = get_files(source_folder, num_files, sort_method='alphabetical')\n",
    "\n",
    "#get_files(source_folder, num_files, sort_method='random')\n",
    "\n",
    "# Loop through the files\n",
    "for filename in filenames:\n",
    "    \n",
    "    print(f\"Next file is {filename}.\")\n",
    "\n",
    "    # set path\n",
    "    file_path = os.path.join(source_folder, filename)\n",
    "\n",
    "    # get training length\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset = json.load(file)['train']\n",
    "    train_len = len(dataset)\n",
    "\n",
    "    print(f\"Training length is {train_len}.\")\n",
    "\n",
    "    # Create the notebook agent, who executes the code\n",
    "    notebook = notebook_create()\n",
    "\n",
    "    # Seed the notebook with useful functions\n",
    "    disable_warnings_code, load_data_function, load_data = notebook_setup(filename, train_or_eval)\n",
    "    notebook.code_executor.nb_append_markdown(f\"## Automated Arc AGI Jupyter Notebook for {filename}\\n\")\n",
    "    notebook.code_executor.nb_append_markdown(\"### HUMAN: Ensure warnings are disabled\")\n",
    "    notebook.code_executor.nb_append_code(disable_warnings_code)\n",
    "    notebook.code_executor.nb_append_markdown(\"### HUMAN: Create data access functions on behalf of the AI agent team\\n\")\n",
    "    notebook.code_executor.nb_append_code(load_data_function)\n",
    "    notebook.code_executor.nb_append_code(load_data)\n",
    "    notebook.code_executor.nb_append_markdown(\"### AI AGENTS: All subsequent notebook entries are by the AI agent team\\n\")\n",
    "\n",
    "    # Create the data scientist agent who proposes the code to be executed by the notebook\n",
    "    datascientist = datascientist_create(model_name)\n",
    "\n",
    "    # Create the task description\n",
    "    task = create_task(filename, train_len, train_or_eval)\n",
    "\n",
    "    # Start the timer\n",
    "    date_time_start = datetime.datetime.now()\n",
    "    print(f\"Starting task {filename} at {date_time_start}\\n\")\n",
    "\n",
    "    # Initiate the task\n",
    "    datascientist.initiate_chat(\n",
    "        recipient     = notebook, \n",
    "        message       = task,\n",
    "        clear_history = True,\n",
    "        max_turns     = 15,   # One turn means one conversation round trip. Prevents chat entering endless loop on problem it cannot solve and racking up token usage\n",
    "        silent        = False # set to True to suppress output\n",
    "    )\n",
    "\n",
    "    # Record task outcome and save the notebook\n",
    "    outcome, date_time_end, duration_txt = notebook_outcome_stats(notebook, filename, file_path, train_or_eval, date_time_start, cwd, model_name, est_method='estimate_1shot')\n",
    "    print(f\"Ending task {filename} at {date_time_end}. Outcome was {outcome}. \\n\")\n",
    "    print(duration_txt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
